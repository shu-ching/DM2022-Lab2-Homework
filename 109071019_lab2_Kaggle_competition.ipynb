{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 楊叔晴\n",
    "\n",
    "Student ID: 109071019\n",
    "\n",
    "GitHub ID: shuching\n",
    "\n",
    "Kaggle name: \n",
    "ShuChing Yang\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-22T11:09:43.157857Z",
     "iopub.status.busy": "2022-11-22T11:09:43.157253Z",
     "iopub.status.idle": "2022-11-22T11:09:43.170930Z",
     "shell.execute_reply": "2022-11-22T11:09:43.168835Z",
     "shell.execute_reply.started": "2022-11-22T11:09:43.157801Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:09:43.175379Z",
     "iopub.status.busy": "2022-11-22T11:09:43.174475Z",
     "iopub.status.idle": "2022-11-22T11:09:43.825359Z",
     "shell.execute_reply": "2022-11-22T11:09:43.824365Z",
     "shell.execute_reply.started": "2022-11-22T11:09:43.175330Z"
    }
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:09:43.827907Z",
     "iopub.status.busy": "2022-11-22T11:09:43.827154Z",
     "iopub.status.idle": "2022-11-22T11:10:16.590794Z",
     "shell.execute_reply": "2022-11-22T11:10:16.589366Z",
     "shell.execute_reply.started": "2022-11-22T11:09:43.827862Z"
    }
   },
   "outputs": [],
   "source": [
    "data_identification = pd.read_csv(\"../input/dm2022-isa5810-lab2-homework/data_identification.csv\")\n",
    "emotion = pd.read_csv(\"../input/dm2022-isa5810-lab2-homework/emotion.csv\")\n",
    "sampleSubmission = pd.read_csv(\"../input/dm2022-isa5810-lab2-homework/sampleSubmission.csv\")\n",
    "raw_data = pd.read_json('../input/dm2022-isa5810-lab2-homework/tweets_DM.json',lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:16.593890Z",
     "iopub.status.busy": "2022-11-22T11:10:16.593242Z",
     "iopub.status.idle": "2022-11-22T11:10:16.614529Z",
     "shell.execute_reply": "2022-11-22T11:10:16.612636Z",
     "shell.execute_reply.started": "2022-11-22T11:10:16.593799Z"
    }
   },
   "outputs": [],
   "source": [
    "# check data\n",
    "data_identification[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:16.619625Z",
     "iopub.status.busy": "2022-11-22T11:10:16.619057Z",
     "iopub.status.idle": "2022-11-22T11:10:16.630714Z",
     "shell.execute_reply": "2022-11-22T11:10:16.629727Z",
     "shell.execute_reply.started": "2022-11-22T11:10:16.619574Z"
    }
   },
   "outputs": [],
   "source": [
    "# check emotion data\n",
    "emotion[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:16.632944Z",
     "iopub.status.busy": "2022-11-22T11:10:16.632265Z",
     "iopub.status.idle": "2022-11-22T11:10:16.650101Z",
     "shell.execute_reply": "2022-11-22T11:10:16.648677Z",
     "shell.execute_reply.started": "2022-11-22T11:10:16.632907Z"
    }
   },
   "outputs": [],
   "source": [
    "# check sampleSubmission data\n",
    "sampleSubmission[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:16.652152Z",
     "iopub.status.busy": "2022-11-22T11:10:16.651654Z",
     "iopub.status.idle": "2022-11-22T11:10:16.676587Z",
     "shell.execute_reply": "2022-11-22T11:10:16.675363Z",
     "shell.execute_reply.started": "2022-11-22T11:10:16.652106Z"
    }
   },
   "outputs": [],
   "source": [
    "# check raw_data data\n",
    "raw_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:16.678628Z",
     "iopub.status.busy": "2022-11-22T11:10:16.678140Z",
     "iopub.status.idle": "2022-11-22T11:10:34.366868Z",
     "shell.execute_reply": "2022-11-22T11:10:34.365507Z",
     "shell.execute_reply.started": "2022-11-22T11:10:16.678580Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the _source columns\n",
    "df = pd.json_normalize(data=raw_data['_source'])\n",
    "\n",
    "# rename column names of source\n",
    "df=df.rename(index=str,columns={\"tweet.text\":\"text\", \"tweet.tweet_id\":\"tweet_id\",\"tweet.hashtags\":\"hashtags\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:34.368858Z",
     "iopub.status.busy": "2022-11-22T11:10:34.368449Z",
     "iopub.status.idle": "2022-11-22T11:10:34.454381Z",
     "shell.execute_reply": "2022-11-22T11:10:34.453115Z",
     "shell.execute_reply.started": "2022-11-22T11:10:34.368824Z"
    }
   },
   "outputs": [],
   "source": [
    "# check normalized and renamed raw data\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:34.457320Z",
     "iopub.status.busy": "2022-11-22T11:10:34.456586Z",
     "iopub.status.idle": "2022-11-22T11:10:38.107669Z",
     "shell.execute_reply": "2022-11-22T11:10:38.106530Z",
     "shell.execute_reply.started": "2022-11-22T11:10:34.457274Z"
    }
   },
   "outputs": [],
   "source": [
    "# add identification the dataframe\n",
    "df=pd.merge(df,data_identification, on=\"tweet_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:38.110148Z",
     "iopub.status.busy": "2022-11-22T11:10:38.109614Z",
     "iopub.status.idle": "2022-11-22T11:10:55.057265Z",
     "shell.execute_reply": "2022-11-22T11:10:55.055721Z",
     "shell.execute_reply.started": "2022-11-22T11:10:38.110100Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean the text\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  #  lowercase text\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)  # remove punctuation\n",
    "    text = \" \".join(text.split())  # remove extra spaces, tabs, and new lines\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:55.059562Z",
     "iopub.status.busy": "2022-11-22T11:10:55.058850Z",
     "iopub.status.idle": "2022-11-22T11:10:55.077587Z",
     "shell.execute_reply": "2022-11-22T11:10:55.076431Z",
     "shell.execute_reply.started": "2022-11-22T11:10:55.059523Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the clean text\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:55.079512Z",
     "iopub.status.busy": "2022-11-22T11:10:55.079017Z",
     "iopub.status.idle": "2022-11-22T11:10:58.673339Z",
     "shell.execute_reply": "2022-11-22T11:10:58.672040Z",
     "shell.execute_reply.started": "2022-11-22T11:10:55.079479Z"
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test dataset\n",
    "train_df = df[df['identification']=='train']\n",
    "train_df = pd.merge(train_df, emotion, on='tweet_id')\n",
    "\n",
    "test_df = df[df['identification']=='test']\n",
    "test_df[\"emotion\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:58.679748Z",
     "iopub.status.busy": "2022-11-22T11:10:58.679297Z",
     "iopub.status.idle": "2022-11-22T11:10:59.804489Z",
     "shell.execute_reply": "2022-11-22T11:10:59.803016Z",
     "shell.execute_reply.started": "2022-11-22T11:10:58.679703Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the identification and hashtags column\n",
    "train_df = train_df.drop(['identification'], axis=1)\n",
    "test_df = test_df.drop(['identification'], axis=1)\n",
    "train_df = train_df.drop(['hashtags'], axis=1)\n",
    "test_df = test_df.drop(['hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:59.806851Z",
     "iopub.status.busy": "2022-11-22T11:10:59.806158Z",
     "iopub.status.idle": "2022-11-22T11:10:59.818844Z",
     "shell.execute_reply": "2022-11-22T11:10:59.817551Z",
     "shell.execute_reply.started": "2022-11-22T11:10:59.806810Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:59.821111Z",
     "iopub.status.busy": "2022-11-22T11:10:59.820733Z",
     "iopub.status.idle": "2022-11-22T11:10:59.840316Z",
     "shell.execute_reply": "2022-11-22T11:10:59.838561Z",
     "shell.execute_reply.started": "2022-11-22T11:10:59.821078Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T02:11:13.687004Z",
     "iopub.status.busy": "2022-11-22T02:11:13.686600Z",
     "iopub.status.idle": "2022-11-22T02:11:13.691727Z",
     "shell.execute_reply": "2022-11-22T02:11:13.690384Z",
     "shell.execute_reply.started": "2022-11-22T02:11:13.686969Z"
    }
   },
   "source": [
    "## 3. Save data in Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:10:59.842880Z",
     "iopub.status.busy": "2022-11-22T11:10:59.842466Z",
     "iopub.status.idle": "2022-11-22T11:11:03.339924Z",
     "shell.execute_reply": "2022-11-22T11:11:03.338842Z",
     "shell.execute_reply.started": "2022-11-22T11:10:59.842845Z"
    }
   },
   "outputs": [],
   "source": [
    "# save to pickle file, for speed\n",
    "train_df.to_pickle(\"train_df.pkl\") \n",
    "test_df.to_pickle(\"test_df.pkl\")\n",
    "\n",
    "# load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:11:03.342292Z",
     "iopub.status.busy": "2022-11-22T11:11:03.341602Z",
     "iopub.status.idle": "2022-11-22T11:11:03.829299Z",
     "shell.execute_reply": "2022-11-22T11:11:03.828364Z",
     "shell.execute_reply.started": "2022-11-22T11:11:03.342250Z"
    }
   },
   "outputs": [],
   "source": [
    "#group to find distribution\n",
    "train_df.groupby(['emotion']).count()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:11:03.831223Z",
     "iopub.status.busy": "2022-11-22T11:11:03.830666Z",
     "iopub.status.idle": "2022-11-22T11:11:04.676187Z",
     "shell.execute_reply": "2022-11-22T11:11:04.674965Z",
     "shell.execute_reply.started": "2022-11-22T11:11:03.831188Z"
    }
   },
   "outputs": [],
   "source": [
    "# the histogram of the data\n",
    "labels = train_df['emotion'].unique()\n",
    "post_total = len(train_df)\n",
    "df1 = train_df.groupby(['emotion']).count()['text']\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "#### 5.1 Count Vectorizer & Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:11:04.678486Z",
     "iopub.status.busy": "2022-11-22T11:11:04.677599Z",
     "iopub.status.idle": "2022-11-22T11:16:13.492055Z",
     "shell.execute_reply": "2022-11-22T11:16:13.490813Z",
     "shell.execute_reply.started": "2022-11-22T11:11:04.678449Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# create a function for the tweet tokenizer from NLTK\n",
    "def tknzr(text):\n",
    "    tt = TweetTokenizer()\n",
    "    return tt.tokenize(text)\n",
    "\n",
    "count_vect = CountVectorizer(max_features=100000, tokenizer=tknzr).fit(train_df['text'])\n",
    "bow_transformed = count_vect.transform(train_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:16:13.494501Z",
     "iopub.status.busy": "2022-11-22T11:16:13.493739Z",
     "iopub.status.idle": "2022-11-22T11:17:02.856968Z",
     "shell.execute_reply": "2022-11-22T11:17:02.855585Z",
     "shell.execute_reply.started": "2022-11-22T11:16:13.494466Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = bow_transformed\n",
    "y_train = train_df['emotion']\n",
    "X_test = count_vect.transform(test_df['text'])\n",
    "y_test = test_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:17:02.859004Z",
     "iopub.status.busy": "2022-11-22T11:17:02.858531Z",
     "iopub.status.idle": "2022-11-22T11:17:02.865330Z",
     "shell.execute_reply": "2022-11-22T11:17:02.863986Z",
     "shell.execute_reply.started": "2022-11-22T11:17:02.858969Z"
    }
   },
   "outputs": [],
   "source": [
    "# take a look at data dimension\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:17:02.866805Z",
     "iopub.status.busy": "2022-11-22T11:17:02.866469Z",
     "iopub.status.idle": "2022-11-22T11:17:02.882070Z",
     "shell.execute_reply": "2022-11-22T11:17:02.880539Z",
     "shell.execute_reply.started": "2022-11-22T11:17:02.866775Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Deal with categorical label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:18:49.245700Z",
     "iopub.status.busy": "2022-11-22T11:18:49.245268Z",
     "iopub.status.idle": "2022-11-22T11:18:55.734438Z",
     "shell.execute_reply": "2022-11-22T11:18:55.733043Z",
     "shell.execute_reply.started": "2022-11-22T11:18:49.245665Z"
    }
   },
   "outputs": [],
   "source": [
    "# deal with label (string -> one-hot)\n",
    "\n",
    "import keras\n",
    "from keras import utils as np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:8])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:20:17.293905Z",
     "iopub.status.busy": "2022-11-22T11:20:17.293396Z",
     "iopub.status.idle": "2022-11-22T11:20:17.301859Z",
     "shell.execute_reply": "2022-11-22T11:20:17.300417Z",
     "shell.execute_reply.started": "2022-11-22T11:20:17.293866Z"
    }
   },
   "outputs": [],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:20:41.023398Z",
     "iopub.status.busy": "2022-11-22T11:20:41.022952Z",
     "iopub.status.idle": "2022-11-22T11:20:41.227885Z",
     "shell.execute_reply": "2022-11-22T11:20:41.226644Z",
     "shell.execute_reply.started": "2022-11-22T11:20:41.023366Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T12:41:47.030779Z",
     "iopub.status.busy": "2022-11-22T12:41:47.029710Z",
     "iopub.status.idle": "2022-11-22T13:15:38.367667Z",
     "shell.execute_reply": "2022-11-22T13:15:38.366427Z",
     "shell.execute_reply.started": "2022-11-22T12:41:47.030724Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger])\n",
    "\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3  Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T11:52:08.292839Z",
     "iopub.status.busy": "2022-11-22T11:52:08.292379Z",
     "iopub.status.idle": "2022-11-22T11:52:15.325802Z",
     "shell.execute_reply": "2022-11-22T11:52:15.324456Z",
     "shell.execute_reply.started": "2022-11-22T11:52:08.292800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# predict!\n",
    "prediction = model.predict(X_test, batch_size=128)\n",
    "prediction = label_decode(label_encoder, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T13:15:58.162168Z",
     "iopub.status.busy": "2022-11-22T13:15:58.161292Z",
     "iopub.status.idle": "2022-11-22T13:15:58.765280Z",
     "shell.execute_reply": "2022-11-22T13:15:58.763960Z",
     "shell.execute_reply.started": "2022-11-22T13:15:58.162125Z"
    }
   },
   "outputs": [],
   "source": [
    "# save as csv\n",
    "test_df['emotion'] = prediction\n",
    "output = test_df[['tweet_id', 'emotion']].copy()\n",
    "output = output.set_axis([\"id\", \"emotion\"], axis=1)\n",
    "output.to_csv(\"submission.csv\", index=False)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
